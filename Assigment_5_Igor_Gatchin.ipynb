{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/igorgatchin1993/assigments/blob/main/Assigment_5_Igor_Gatchin.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following assignment consists of a theoretical part (learning portfolio) and a practical part (assignment). The goal is to build a classification model that predicts from which subject area a certain abstract originates. The plan would be that next week we will discuss your learnings from the theory part, that means you are relatively free to fill your Learning Portfolio on this new topic and in two weeks we will discuss your solutions of the Classification Model.\n",
        "\n",
        "#Theory part (filling your Learning Portfolio, May 10)\n",
        "\n",
        "In preparation for the practical part, I ask you to familiarize yourself with the following resources in the next week:\n",
        "\n",
        "1) Please watch the following video:\n",
        "\n",
        "https://course.fast.ai/Lessons/lesson4.html\n",
        "\n",
        "You are also welcome to watch the accompanying Kaggle notebook if you like the video.\n",
        "\n",
        "2) In addition to the video, I recommend you to read the first chapters of the course\n",
        "\n",
        "https://huggingface.co/learn/nlp-course/chapter1/1\n",
        "\n",
        "\n",
        "Try to understand principle processes and log them in your learning portfolio! A few suggestions: What is a pre-trained NLP model? How do I load them? What is tokenization? What does fine-tuning mean? What types of NLP Models are there? What possibilities do I have with the Transformers package? etc...\n",
        "\n",
        "Question: A few suggestions: What is a pre-trained NLP model?\n",
        "\n",
        "\n",
        "Answer:\n",
        "The video \"FastAI Lesson 4: NLP Deep Dive\" introduces the principles and techniques of natural language processing (NLP), including text classification, tokenization, language modeling, and transfer learning. The video also discusses how to use pre-trained language models, such as ULMFiT, to improve the performance of NLP models. The accompanying Kaggle notebook provides hands-on exercises and examples of NLP applications using the FastAI library.\n",
        "\n",
        "The first chapter of the Hugging Face NLP course introduces the basics of NLP, including tokenization, text normalization, and part-of-speech tagging. The chapter also explains the concept of pre-training in NLP, which involves training a language model on a large corpus of text data to learn general linguistic features that can be transferred to downstream tasks. The chapter discusses different pre-trained models, such as BERT and GPT, and how to fine-tune them for specific NLP tasks.\n",
        "\n",
        "Overall, a pre-trained NLP model is a language model that has been trained on a large corpus of text data using unsupervised learning techniques. The model learns to represent language in a way that captures semantic and syntactic information, which can be used for various NLP tasks such as text classification, sentiment analysis, and question answering. Pre-trained models can be fine-tuned on specific tasks with smaller labeled datasets, allowing for efficient and accurate NLP applications.\n",
        "\n",
        "Question: How do I load them? What is tokenization?\n",
        "\n",
        "Answer: \n",
        "\n",
        "To load a pre-trained NLP model, you can use a library like Hugging Face Transformers, which provides a simple and flexible API for working with pre-trained NLP models. \n",
        "\n",
        "Tokenization is the process of splitting a piece of text into smaller units called tokens. Tokens are usually words, but they can also be subwords or characters depending on the tokenization algorithm used. Tokenization is a crucial step in NLP because it enables us to represent text as a sequence of tokens that can be processed by machine learning models.\n",
        "\n",
        "\n",
        "Question: What does fine-tuning mean?\n",
        "\n",
        "Answer:\n",
        "In the context of natural language processing (NLP), fine-tuning refers to the process of training a pre-trained language model on a specific task using a smaller labeled dataset. Fine-tuning allows the model to adapt to the specific domain and style of the new task, while still leveraging the general linguistic knowledge learned from the pre-training stage.\n",
        "\n",
        "When a language model is pre-trained, it learns to represent language in a general and unsupervised way, without being explicitly trained on any specific task. However, pre-trained models can be fine-tuned for specific tasks such as text classification, sentiment analysis, or question answering, by feeding them with labeled examples of the target task.\n",
        "\n",
        "The process of fine-tuning involves initializing the pre-trained model with its learned weights and then training it on the new task using the labeled data. During the fine-tuning process, the model adjusts its parameters to fit the new task while still preserving the general linguistic knowledge it learned during pre-training. Fine-tuning often requires fewer labeled examples than training a model from scratch and can lead to better performance on the target task.\n",
        "\n",
        "Question:What types of NLP Models are there?\n",
        "\n",
        "Answer:\n",
        "\n",
        "There are several types of NLP models, each with its own strengths and weaknesses depending on the specific task and data.\n",
        "\n",
        "Rule-based models: These models use hand-crafted rules and patterns to analyze text. They are often used for tasks such as part-of-speech tagging or named entity recognition, where the rules can be defined based on the grammar and syntax of the language. However, rule-based models can be limited by the complexity of the rules and may not generalize well to new data.\n",
        "\n",
        "Statistical models: These models use statistical algorithms to learn patterns in the data and make predictions. Examples include hidden Markov models, maximum entropy models, and conditional random fields. Statistical models can be effective for tasks such as text classification or machine translation, but they can be computationally expensive and require large amounts of labeled data.\n",
        "\n",
        "Neural network models: These models use artificial neural networks to learn representations of text and make predictions. Examples include feedforward networks, recurrent neural networks, and convolutional neural networks.\n",
        "\n",
        "Neural network models have achieved state-of-the-art results on many NLP tasks, especially with the advent of pre-trained language models like BERT and GPT-2. However, they can be computationally expensive and require large amounts of data for training.\n",
        "\n",
        "Hybrid models: These models combine multiple approaches, such as combining rule-based and statistical models or neural network models with traditional machine learning algorithms. Hybrid models can leverage the strengths of different approaches and often achieve better performance than individual models alone.\n",
        "\n",
        "In addition to these types of models, there are also pre-trained language models that have been trained on large amounts of text data using neural network models, such as BERT, GPT-2, and RoBERTa. These models can be fine-tuned on specific NLP tasks with smaller labeled datasets, which can save time and improve performance compared to training models from scratch.\n",
        "\n",
        "Question: What possibilities do I have with the Transformers package? \n",
        "\n",
        "Answer:\n",
        "\n",
        "The Transformers package is a powerful library for natural language processing (NLP) that provides many possibilities for working with pre-trained language models, fine-tuning them on specific tasks, and building custom models for NLP tasks. Here are some of the key possibilities provided by the Transformers package:\n",
        "\n",
        "Loading pre-trained models: The Transformers package provides access to a wide range of pre-trained language models, including BERT, GPT-2, RoBERTa, and more. These models can be loaded and used for various NLP tasks, such as text classification, named entity recognition, question answering, and more.\n",
        "\n",
        "Tokenization: The Transformers package provides various tokenizers for converting raw text into tokens that can be used as input to a pre-trained model. These tokenizers support various tokenization strategies, including byte-level BPE (Byte Pair Encoding) and WordPiece tokenization.\n",
        "\n",
        "Fine-tuning pre-trained models: The Transformers package provides an easy way to fine-tune pre-trained models on specific tasks using a few lines of code. Fine-tuning involves training the pre-trained model on a smaller labeled dataset for a specific task, such as sentiment analysis or text classification.\n",
        "\n",
        "Custom model building: The Transformers package provides a high-level API for building custom models using pre-trained language models as building blocks. This allows users to easily build models for specific NLP tasks using pre-trained models and fine-tuning them on labeled data.\n",
        "\n",
        "Integration with other NLP tools: The Transformers package provides integration with other NLP tools such as spaCy, Hugging Face's Datasets library, and PyTorch Lightning. This allows users to build end-to-end NLP pipelines that can be used for various applications.\n",
        "\n",
        "Access to pre-trained embeddings: The Transformers package provides access to pre-trained word embeddings such as GloVe and FastText. These embeddings can be used for various NLP tasks such as text classification and named entity recognition.\n",
        "\n",
        "Overall, the Transformers package provides a wide range of possibilities for working with pre-trained language models, fine-tuning them on specific tasks, and building custom models for NLP tasks. It has become a popular library for NLP researchers and practitioners due to its ease of use and flexibility.\n",
        "\n",
        "\n",
        "#Practical part (Assignment, May 17)\n",
        "\n",
        "1) Preprocessing: The data which I provide as zip in Olat must be processed first, that means we need a table which has the following form:\n",
        "\n",
        "Keywords | Title | Abstract | Research Field\n",
        "\n",
        "The research field is determined by the name of the file.\n",
        "\n",
        "2) We need a training dataset and a test dataset. My suggestion would be that for each research field we use the first 5700 lines for the training dataset and the last 300 lines for the test dataset. Please stick to this because then we can compare our models better!\n",
        "\n",
        "3) Please use a pre-trained model from huggingface to build a classification model that tries to predict the correct research field from the 26. Please calculate the accuracy and the overall accuracy for all research fields. If you solve this task in a group, you can also try different pre-trained models. In addition to the abstracts, you can also see if the model improves if you include keywords and titles.\n",
        "\n",
        "Some links, which can help you:\n",
        "\n",
        "https://huggingface.co/docs/transformers/training\n",
        "\n",
        "https://huggingface.co/docs/transformers/tasks/sequence_classification\n",
        "\n",
        "One last request: Please always use PyTorch and not TensorFlow!"
      ],
      "metadata": {
        "id": "Rv37EvemaCce"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUi9az-cZ9zq"
      },
      "outputs": [],
      "source": [
        "#YOUR TASK"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Addition: Accuracy measures whether the research field with the highest probability value matches the target. With 26 research fields, it would also be interesting to know if the correct target is at least among the three highest probability values.\n",
        "\n",
        "$\\begin{pmatrix} A\\\\ B \\\\ C \\\\D \\\\E \\end{pmatrix} = \\begin{pmatrix} 0.1\\\\ 0.95 \\\\ 0.5 \\\\0.2 \\\\0.3 \\end{pmatrix} → \\text{Choice}_1 = B, \\text{Choice}_3 = B,C,E$"
      ],
      "metadata": {
        "id": "Up3aCw__4f4d"
      }
    }
  ]
}