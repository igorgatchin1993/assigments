{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/igorgatchin1993/assigments/blob/main/assigment_3_Igor_Gatchin.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#hide\n",
        "! [ -e /content ] && pip install -Uqq fastbook\n",
        "import fastbook\n",
        "from fastai.vision.all import *\n",
        "from fastbook import *\n",
        "fastbook.setup_book()"
      ],
      "metadata": {
        "id": "HhmY7I5M8VJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Artificial Neural Networks\n",
        "\n",
        "Please read the introdcution of neuronal networks of the book *Hands-On Machine Learning with Scikit-Learn, Keras & TensorFlow*, p. 299-316.\n",
        "\n",
        "1) Why have neural networks, even though they were invented early on, only now caught on?\n",
        "\n",
        "Neural networks, a type of machine learning model inspired by the structure and function of the human brain, have been around for several decades, with the earliest concepts dating back to the 1940s. However, it is only in recent years that neural networks have gained widespread popularity and become a dominant approach in many areas of machine learning and artificial intelligence.\n",
        "\n",
        "There are several reasons why neural networks have gained traction in recent times:\n",
        "\n",
        "Advances in computational power: The increased availability of high-performance computing resources, including GPUs (Graphics Processing Units) and specialized hardware for deep learning, has significantly accelerated the training and inference speed of neural networks. This has made it possible to train and deploy much larger and more complex neural networks, leading to improved performance in various tasks.\n",
        "\n",
        "Big data: The explosion of data in recent years, often referred to as \"big data,\" has provided neural networks with ample training examples, allowing them to learn complex patterns and representations from large datasets. This has been instrumental in the success of neural networks in areas such as image and speech recognition, natural language processing, and recommendation systems.\n",
        "\n",
        "Algorithmic advancements: There have been significant advancements in the field of neural network research, including the development of new activation functions, optimization algorithms, regularization techniques, and network architectures. These advancements have improved the stability, convergence, and generalization capabilities of neural networks, making them more robust and effective in real-world applications.\n",
        "\n",
        "Availability of open-source libraries and tools: The availability of popular open-source libraries and tools, such as TensorFlow, Keras, and PyTorch, has democratized the use of neural networks, making them accessible to a wide range of developers and researchers. These libraries provide high-level APIs and pre-implemented neural network architectures, simplifying the development and experimentation process.\n",
        "\n",
        "Industry adoption: Neural networks have been adopted by various industries, including technology, finance, healthcare, automotive, and more. Their success in practical applications has spurred further interest and investment in neural network research and development, leading to advancements and innovation in the field.\n",
        "\n",
        "In summary, a combination of advances in computational power, big data availability, algorithmic advancements, availability of open-source tools, and industry adoption has contributed to the recent resurgence of neural networks and their widespread adoption in various domains of machine learning and artificial intelligence.\n",
        "\n",
        "2) What is a percepton and a threshold logic unit (TLU)? Try to define a linear function and a step function of your choice, use some values of your choice and explain what might be the result of the percepton. (maybe using max. two TLU's)\n",
        "\n",
        "A perceptron is a type of artificial neural network model that can perform binary classification tasks. It consists of a single layer of output nodes, and takes a weighted sum of input features, applies an activation function, and produces an output prediction. The activation function used in a perceptron is typically a step function, which produces a binary output based on whether the weighted sum exceeds a threshold value.\n",
        "\n",
        "A threshold logic unit (TLU), also known as a threshold gate or McCulloch-Pitts neuron, is a simplified model of a neuron that computes a weighted sum of input features and applies a step function as its activation function. It produces an output based on whether the weighted sum exceeds a predefined threshold value.\n",
        "\n",
        "A linear function is a mathematical function that produces a straight-line relationship between input and output. It can be represented as f(x) = wx + b, where w is the weight, x is the input, and b is the bias term. The output is a continuous value that can be positive or negative.\n",
        "\n",
        "A step function is a mathematical function that produces a binary output based on whether the input exceeds a threshold value. It can be represented as f(x) = 1 if x > threshold, otherwise f(x) = 0. The output is discrete and takes on only two values, typically 0 or 1.\n",
        "\n",
        "Let's consider an example where we have a perceptron with two input features, x1 and x2, and weights w1 = 0.5, w2 = -0.3, and a bias term b = 0.2. The threshold for the step function is set at 0.5.\n",
        "\n",
        "For input values x1 = 0.7 and x2 = -0.4, the weighted sum in the perceptron would be:\n",
        "\n",
        "weighted sum = (w1 * x1) + (w2 * x2) + b\n",
        "= (0.5 * 0.7) + (-0.3 * -0.4) + 0.2\n",
        "= 0.35 + 0.12 + 0.2\n",
        "= 0.67\n",
        "\n",
        "Since the weighted sum (0.67) exceeds the threshold (0.5), the perceptron would produce an output of 1, indicating a positive class prediction.\n",
        "\n",
        "Now, let's consider a scenario where we have two TLUs, TLU1 and TLU2, each with different thresholds. TLU1 has weights w1 = 0.3, w2 = -0.2, and threshold = 0.4, while TLU2 has weights w1 = -0.1, w2 = 0.2, and threshold = -0.3.\n",
        "\n",
        "For the same input values x1 = 0.7 and x2 = -0.4, we can compute the outputs of TLU1 and TLU2 as follows:\n",
        "\n",
        "Output of TLU1 = f(w1 * x1 + w2 * x2 - threshold)\n",
        "= f(0.3 * 0.7 + (-0.2 * -0.4) - 0.4)\n",
        "= f(0.21 + 0.08 - 0.4)\n",
        "= f(-0.11)\n",
        "= 0\n",
        "\n",
        "Output of TLU2 = f(w1 *\n",
        "\n",
        "3) What is a fully connected layer and a output layer? Why can we easily combine the equations of multiple instances into a fully connected layer?\n",
        "\n",
        "A fully connected layer, also known as a dense layer, is a type of layer in a neural network where each node is connected to all the nodes in the previous layer, and vice versa. This means that the input to each node is a weighted sum of the outputs of all the nodes in the previous layer, and an activation function is applied to the weighted sum to produce the output of the node. Fully connected layers are commonly used in neural networks for tasks such as image classification, speech recognition, and natural language processing.\n",
        "\n",
        "An output layer, also known as the top or final layer of a neural network, is responsible for producing the final predictions or outputs of the model. The activation function used in the output layer depends on the type of problem being solved. For binary classification tasks, a sigmoid activation function is often used to produce a probability output between 0 and 1, where values above a certain threshold are predicted as the positive class and values below the threshold are predicted as the negative class. For multi-class classification tasks, a softmax activation function is commonly used to produce a probability distribution over multiple classes, with the class with the highest probability being predicted as the final output.\n",
        "\n",
        "The equations of multiple instances can be easily combined into a fully connected layer because the weights and biases of the layer are shared across all instances. This means that the same set of weights and biases are used for all the instances in the layer, and each instance contributes to the weighted sum of the layer based on its own input features. This allows the neural network to learn complex patterns and relationships from the combined inputs of all instances, leading to more accurate predictions or outputs. The ability to share weights and biases across instances is one of the reasons why fully connected layers are effective in capturing complex patterns in data.\n",
        "\n",
        "4) What problem did Marvin Minsky and Seymour Paper highlight that perceptrons could not solve? What is a possible solution?\n",
        "\n",
        "Marvin Minsky and Seymour Papert highlighted that perceptrons, a type of neural network, were not capable of solving problems that were not linearly separable. In other words, perceptrons could not learn to classify inputs that were not linearly separable using a single layer of neurons with a step activation function.\n",
        "\n",
        "A possible solution to this problem is to use multi-layer perceptrons (MLPs), which are neural networks with multiple layers of interconnected nodes. By introducing hidden layers with nonlinear activation functions, MLPs can learn to capture more complex patterns in the data, including those that are not linearly separable. The hidden layers allow for the transformation of the input data into higher-dimensional representations, which can help in learning more intricate decision boundaries.\n",
        "\n",
        "In addition to using MLPs, other types of neural networks such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs) have also been developed to address the limitations of perceptrons and are widely used in various machine learning tasks. These networks utilize different architectures and types of layers to capture different types of patterns and relationships in data, allowing them to solve more complex problems that cannot be solved by perceptrons alone.\n",
        "\n",
        "5) What is a deep neuronal network? What are hidden layers? What means feedforward neural network (FNN).\n",
        "\n",
        "A deep neural network, also known as a deep neural network, is a type of artificial neural network that consists of multiple layers of interconnected nodes or neurons. These layers are organized into an input layer, one or more hidden layers, and an output layer. Deep neural networks are capable of learning complex patterns and representations from data, making them well-suited for tasks such as image recognition, natural language processing, and speech recognition.\n",
        "\n",
        "Hidden layers in a neural network refer to the layers that are located between the input layer and the output layer. These layers are called \"hidden\" because they do not have direct connections to the input or output of the network and their outputs are not directly visible. Each node or neuron in a hidden layer receives inputs from the nodes in the previous layer, performs a computation using an activation function, and produces an output that serves as the input to the next layer. Hidden layers allow the network to learn and capture complex features or representations of the input data, which are then used for making predictions or producing outputs.\n",
        "\n",
        "A feedforward neural network (FNN) is a type of neural network in which information flows only in one direction, from the input layer to the output layer. The input data is passed through one or more hidden layers, with each layer applying a nonlinear transformation to the inputs using an activation function. The outputs from the hidden layers are then passed to the output layer, where the final predictions or outputs are produced. FNNs are the most common type of neural networks and are often used for tasks such as image classification, speech recognition, and regression.\n",
        "\n",
        "6) Try to explain how backpropagation works! (In Addition, you can have a look to the following example, which tries manually to compute the backprogation of a simple linear network. https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/ OR you can also read through the Google Colab [04_mnist_basics.ipynb](https://colab.research.google.com/github/fastai/fastbook/blob/master/04_mnist_basics.ipynb#scrollTo=t1DK6o-gckCy))\n",
        "\n",
        "\n",
        "Backpropagation is a widely used algorithm for training neural networks. It is a supervised learning algorithm that involves adjusting the weights of the network in order to minimize the error between the predicted outputs and the actual outputs for a given set of training data. Here's a high-level overview of how backpropagation works:\n",
        "\n",
        "Forward Pass: The input data is fed through the neural network in the forward direction. The inputs are multiplied by the weights of the connections between the neurons and passed through an activation function to produce the outputs of each neuron. This process is repeated for each layer in the network until the final output is produced.\n",
        "\n",
        "Compute Loss: The error or loss between the predicted outputs and the actual outputs is computed. This is typically done using a loss function such as mean squared error (MSE) or cross-entropy.\n",
        "\n",
        "Backward Pass: The computed loss is then propagated backward through the network from the output layer to the input layer. The gradient of the loss with respect to the outputs of each neuron is computed. This gradient represents the sensitivity of the loss to changes in the outputs of the neurons.\n",
        "\n",
        "Update Weights: The gradients are then used to update the weights of the connections between the neurons in the network. The weights are adjusted in a way that minimizes the loss, typically using an optimization algorithm such as gradient descent. The magnitude of the weight updates is determined by the learning rate, which is a hyperparameter that controls the step size of the optimization process.\n",
        "\n",
        "Repeat: Steps 1-4 are repeated for a number of iterations or epochs, with the network continuously updating its weights to minimize the error between the predicted and actual outputs. This process continues until the network converges to a set of weights that produce satisfactory results on the training data.\n",
        "\n",
        "The backpropagation algorithm allows the neural network to learn from the training data by iteratively adjusting its weights based on the computed gradients. This allows the network to update its internal representations and learn to make better predictions as the training progresses.\n",
        "\n",
        "7) Why do we need activation functions, wouldn't it be easier just using linear functions?\n",
        "\n",
        "Activation functions are an essential component of neural networks, and they serve an important purpose in the overall functioning of the network. While it is possible to use linear activation functions in neural networks, there are several key reasons why nonlinear activation functions are preferred:\n",
        "\n",
        "Non-linearity: Linear functions are limited to producing only linear relationships between input and output, which can severely limit the capacity of a neural network to learn complex patterns and representations in data. \n",
        "Neural networks are designed to capture non-linear relationships in data, and activation functions introduce non-linearity into the network, enabling it to learn more complex and intricate patterns.\n",
        "\n",
        "Representation Power: Non-linear activation functions allow neural networks to approximate any continuous function to arbitrary accuracy, as per the Universal Approximation Theorem. This means that neural networks with non-linear activation functions have the potential to learn highly complex mappings between inputs and outputs, making them more powerful and expressive.\n",
        "\n",
        "Gradient Flow: Activation functions also play a critical role in the backpropagation algorithm, which is used for training neural networks. During backpropagation, gradients (i.e., derivatives) of the loss with respect to the network parameters are computed and used to update the weights. \n",
        "\n",
        "Non-linear activation functions help propagate these gradients more effectively through the network, ensuring that the model can learn from the data and update its weights properly.\n",
        "\n",
        "Robustness and Invariance: Non-linear activation functions can introduce robustness and invariance properties into neural networks. For example, activation functions such as ReLU (Rectified Linear Unit) can help the network to be more robust to noise and outliers in the data, as they can \"shut off\" certain neurons that have negative inputs. This can help the network to learn more robust representations of the data.\n",
        "In summary, activation functions are necessary in neural networks because they introduce non-linearity, allow for powerful function approximation, facilitate effective gradient flow during training, and can provide robustness and invariance properties to the network. Linear activation functions would be limited in their ability to learn complex patterns and representations, and therefore nonlinear activation functions are essential for the success of neural networks in many real-world applications.\n",
        "\n",
        "## Ideas for the learning portfolio: \n",
        "\n",
        "1) For example, you could train a single TLU to classify iris flowers based on petal length and width in the !!!pyTorch!! environment.\n",
        "\n",
        "An example of how you could train a single Threshold Logic Unit (TLU) to classify iris flowers based on petal length and width using PyTorch, which is a popular deep learning framework:\n",
        "\n",
        "\n",
        "\n",
        "2) You could add to our king county housepricing ML project a neuronal network and compare it to the other models. "
      ],
      "metadata": {
        "id": "_Rdj49uwjuoU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "iris = datasets.load_iris()\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define the TLU model\n",
        "class TLU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TLU, self).__init__()\n",
        "        self.weight = nn.Parameter(torch.randn(2, 1))  # Learnable weight parameter\n",
        "        self.bias = nn.Parameter(torch.randn(1))  # Learnable bias parameter\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Compute the linear combination of inputs and weight, and apply the threshold activation function\n",
        "        linear_combination = torch.matmul(x, self.weight) + self.bias\n",
        "        output = torch.where(linear_combination > 0, torch.tensor([1.0]), torch.tensor([0.0]))\n",
        "        return output\n",
        "\n",
        "# Create the dataset\n",
        "# Assume X is a tensor of shape (N, 2) where N is the number of samples,\n",
        "# and each row of X represents the petal length and width for a single iris flower\n",
        "# Assume y is a tensor of shape (N, 1) where each element of y represents the class label (0 or 1)\n",
        "X = torch.tensor([[5.1, 3.5], [4.9, 3.0], [5.8, 2.6], [6.0, 3.4], [5.0, 3.5], [6.7, 3.1]])\n",
        "y = torch.tensor([[0], [0], [0], [1], [1], [1]])\n",
        "\n",
        "# Initialize the TLU model and the optimizer\n",
        "model = TLU()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 1000\n",
        "for epoch in range(num_epochs):\n",
        "    # Forward pass\n",
        "    predictions = model(X)\n",
        "    \n",
        "    # Compute the loss (binary cross entropy)\n",
        "    loss = nn.BCELoss()(predictions, y.float())\n",
        "    \n",
        "    # Backward pass\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    \n",
        "    # Update the model parameters\n",
        "    optimizer.step()\n",
        "    \n",
        "    # Print the loss for monitoring\n",
        "    if epoch % 100 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluate the trained model\n",
        "with torch.no_grad():\n",
        "    test_X = torch.tensor([[5.2, 3.6], [4.8, 3.2], [6.1, 2.8]])\n",
        "    test_predictions = model(test_X)\n",
        "    test_predictions = torch.round(test_predictions)  # Round the predictions to obtain class labels (0 or 1)\n",
        "    print(f'Test Predictions: {test_predictions.squeeze().tolist()}')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4tF8YDV3T91n"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "An example of how I could incorporate a neural network into the King County house pricing machine learning project and compare it to other models:\n",
        "\n",
        "1) Prepare the Data: Load and preprocess the King County house pricing dataset, including handling missing values, categorical features, and feature scaling.\n",
        "\n",
        "2) Define the Neural Network Model: Use a deep neural network architecture with multiple hidden layers and activation functions. You can use popular deep learning libraries such as TensorFlow or PyTorch to define your neural network model. For example, in PyTorch, you can define your neural network model as follows:"
      ],
      "metadata": {
        "id": "dde1mRJNTBK6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class HousePriceNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(HousePriceNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "N5rWV8dXS7uW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) Train the Neural Network Model: Split the dataset into training and validation sets, initialize the neural network model, define the loss function (e.g., mean squared error) and the optimizer (e.g., stochastic gradient descent), and train the model using the training data. Monitor the training progress using validation data and adjust hyperparameters (e.g., learning rate, batch size, number of hidden layers/nodes) as needed to optimize model performance.\n",
        "\n",
        "4) Evaluate the Neural Network Model: Once the model is trained, evaluate its performance on the test set using appropriate evaluation metrics (e.g., mean squared error, R-squared). Compare the performance of the neural network model with other models you have implemented in your King County house pricing project, such as linear regression, decision tree, or random forest. You can use techniques such as cross-validation to get a more robust estimate of model performance.\n",
        "\n",
        "5) Fine-tune and Optimize the Neural Network Model: Experiment with different hyperparameter settings, architectures (e.g., number of hidden layers/nodes), and activation functions to further optimize the performance of the neural network model. You can also consider techniques such as regularization (e.g., L1 or L2 regularization), dropout, or batch normalization to improve the model's generalization and robustness.\n",
        "\n",
        "6) Interpret and Explain Model Results: Finally, interpret and explain the results of your neural network model, including the important features, feature interactions, and any other insights gained from the model. Visualize the model's predictions, residuals, and other diagnostic plots to gain a better understanding of the model's performance and limitations."
      ],
      "metadata": {
        "id": "tIXyZ1QBS_Q7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A traditional approach: training a digit classifier and learning pyTorch tensors.\n",
        "\n",
        "For this assignment, I ask you to read the Google Colab [04_mnist_basics.ipynb](https://colab.research.google.com/github/fastai/fastbook/blob/master/04_mnist_basics.ipynb#scrollTo=t1DK6o-gckCy) to the beginning of the chapter *Stochastic Gradient Descent (SGD)*. \n",
        "\n",
        "First, try to summarize what we know about pyTorch tensors by trying to predict whether we have a 1 or a 7 in the MNIST dataset using a traditional rule-based programming approach. Therefore use pyTorch tensors for the entire tasks and fulfill the following steps:\n",
        "\n",
        "1) Randomly split the MNIST dataset (1 and 7) into a training dataset and a test dataset in a ratio of 80:20.\n",
        "\n",
        "2) Instead of using an optimal 1 or 7 with the mean over the training dataset, try to calculate the sum of the distances to all instances in the training set for each instance in the test dataset. You can use the L2 norm. \n",
        "\n",
        "3) For each instance in the test set, decide if it is a 1 or 7 and calculate the precision.\n",
        "\n",
        "Do we get a similar good result?\n"
      ],
      "metadata": {
        "id": "h6OwXNEeed93"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "# Define a transform to normalize the data\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "# Load the MNIST dataset\n",
        "mnist = datasets.MNIST('data/', train=True, download=True, transform=transform)\n",
        "\n",
        "# Filter the dataset to keep only samples with digits 1 or 7\n",
        "mnist_17 = [data for data in mnist if data[1] == 1 or data[1] == 7]\n",
        "\n",
        "# Calculate the number of samples for training and test datasets\n",
        "total_samples = len(mnist_17)\n",
        "train_samples = int(0.8 * total_samples)\n",
        "test_samples = total_samples - train_samples\n",
        "\n",
        "# Randomly split the dataset into training and test datasets\n",
        "train_dataset, test_dataset = random_split(mnist_17, [train_samples, test_samples])\n",
        "\n",
        "# Create data loaders for training and test datasets\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JktIP_GS49Nr",
        "outputId": "0a33be4b-090b-41af-d739-832730d43de7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 105336834.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 79173656.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 26817006.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 11279176.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the code above, we first define a transform to normalize the data, and then load the MNIST dataset using PyTorch's datasets.MNIST class. Next, we filter the dataset to keep only samples that contain the digits 1 or 7. We then use the random_split function to randomly split the filtered dataset into training and test datasets in the ratio of 80:20. Finally, we create data loaders for the training and test datasets using PyTorch's DataLoader class, which will allow us to efficiently load and process the data during training and evaluation."
      ],
      "metadata": {
        "id": "RvOWAK3e4_xo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Calculate sum of distances using L2 norm\n",
        "def calculate_sum_distances(test_loader, train_loader):\n",
        "    sum_distances = []\n",
        "    for test_data in test_loader:\n",
        "        test_images, _ = test_data\n",
        "        sum_distance = 0\n",
        "        for train_data in train_loader:\n",
        "            train_images, _ = train_data\n",
        "            distances = torch.norm(test_images - train_images, dim=(1,2,3)) # Calculate L2 norm\n",
        "            sum_distance += torch.sum(distances)\n",
        "        sum_distances.append(sum_distance.item())\n",
        "    return sum_distances\n",
        "\n",
        "# Calculate sum of distances for test instances\n",
        "sum_distances = calculate_sum_distances(test_loader, train_loader)\n",
        "\n",
        "# Print sum of distances for each test instance\n",
        "for i, sum_distance in enumerate(sum_distances):\n",
        "    print(\"Sum of distances for test instance {}: {}\".format(i+1, sum_distance))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "CKc6VnAG5dx2",
        "outputId": "586b5f23-e86c-4dc7-f577-8a434cf8461e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-828ebd96af76>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Calculate sum of distances for test instances\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0msum_distances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_sum_distances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Print sum of distances for each test instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-828ebd96af76>\u001b[0m in \u001b[0;36mcalculate_sum_distances\u001b[0;34m(test_loader, train_loader)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtrain_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mtrain_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mdistances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_images\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtrain_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Calculate L2 norm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0msum_distance\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0msum_distances\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum_distance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/functional.py\u001b[0m in \u001b[0;36mnorm\u001b[0;34m(input, p, dim, keepdim, out, dtype)\u001b[0m\n\u001b[1;32m   1508\u001b[0m                 \u001b[0m_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1510\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1511\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1512\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: linalg.matrix_norm: dim must be a 2-tuple. Got 1 2 3"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the code above, we define a function calculate_sum_distances that takes in the test and train data loaders as input. Within this function, we iterate over each test instance and calculate the L2 norm (Euclidean distance) between the test instance and each training instance using PyTorch's torch.norm function. We then sum up the distances from the test instance to all training instances using torch.sum, and store the sum of distances for each test instance in a list sum_distances. Finally, we print out the sum of distances for each test instance."
      ],
      "metadata": {
        "id": "q5uzsLTj5krQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Load the trained model\n",
        "# Note: This assumes that you have already trained and saved the model in the notebook\n",
        "# Refer to the notebook for the specific model architecture and how to save and load the model\n",
        "model = torch.load('model.pth') # Load the trained model\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Initialize variables for precision calculation\n",
        "num_correct = 0\n",
        "num_predicted = 0\n",
        "\n",
        "# Iterate over each instance in the test set\n",
        "for test_data in test_loader:\n",
        "    test_images, test_labels = test_data\n",
        "    # Forward pass through the model to get predicted labels\n",
        "    with torch.no_grad():\n",
        "        output = model(test_images)\n",
        "        _, predicted_labels = torch.max(output, dim=1)\n",
        "    # Update precision count\n",
        "    num_correct += torch.sum(predicted_labels == test_labels).item()\n",
        "    num_predicted += test_labels.size(0)\n",
        "\n",
        "# Calculate precision\n",
        "precision = num_correct / num_predicted\n",
        "\n",
        "# Print the overall precision\n",
        "print(\"Overall Precision: {:.4f}\".format(precision))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "uGCfh6si5nJS",
        "outputId": "374c7375-4851-46a5-bd87-6677f7dc0723"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-6c00d1cf8505>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Note: This assumes that you have already trained and saved the model in the notebook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Refer to the notebook for the specific model architecture and how to save and load the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model.pth'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Load the trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Set the model to evaluation mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 791\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    792\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'model.pth'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the code above, we load the trained model from the saved checkpoint file using torch.load(). We set the model to evaluation mode using model.eval() to disable dropout and other layers that behave differently during training and testing. We then iterate over each instance in the test set and pass it through the model to obtain predicted labels. We compare the predicted labels with the ground truth labels using torch's torch.max() and calculate the number of correct predictions. Finally, we calculate the overall precision by dividing the number of correct predictions by the total number of predicted instances, and print the overall precision value."
      ],
      "metadata": {
        "id": "iy4KMFjZ6Lkx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stochastic Gradient Descent (SGD)\n",
        "\n",
        "For this exercise I ask you to read the chapter Stochastic Gradient Descent (SGD) from the Google Colab 04_mnist_basics.ipynb in paralell. The chapter starts with a single TLU, compare p. 304 in \"Hands on Machine Learning\". Go through all 7 steps which are an easy example of how Stochastic Gradient Descent works.\n",
        "\n",
        "Our goal is to train a single TLU, which can decide if one number is larger then the other one. Therefore we create 100 random pairs with pyTorch and create a target vector which is eather 1 or 0.\n"
      ],
      "metadata": {
        "id": "ETcE9B9rdcEI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn((100, 2))\n",
        "y = torch.where(x[:,0] > x[:,1], 1.0, 0.0)\n",
        "print(x)\n",
        "print(y)"
      ],
      "metadata": {
        "id": "17qLyDnbpSbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your task is to create a function f that is a single TLU, meaning that it summarizes x with weights a, b, c:\n",
        "\n",
        "$ax_0+bx_1+c$\n",
        "\n",
        "In Addition we are using a *sigmoid()* function as step function.\n",
        "\n",
        "$f = \\text{sigmoid}(ax_0+bx_1+c)$"
      ],
      "metadata": {
        "id": "z267w4G48rxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f(x, params):\n",
        "    a,b,c = params\n",
        "    return #YOUR TASK \n",
        "\n",
        "print(f(x, [3,-2,1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_NvBnCGoLPx",
        "outputId": "f15684b2-c773-47ac-a705-db5337e3ec08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.0138, 0.9517, 0.9690, 0.5570, 0.9972, 0.9988, 0.8296, 0.8440, 0.9849, 0.2474, 0.0599, 0.0372, 0.9726, 0.9878, 0.4036, 0.3938, 0.0367, 0.9965, 0.7762, 0.9998, 0.0793, 0.9431, 0.5434, 0.9998,\n",
            "        0.9779, 0.0594, 0.4891, 0.0048, 0.4292, 0.6232, 0.6732, 0.0202, 0.4517, 0.9995, 0.2007, 0.2726, 0.8806, 0.0339, 0.7862, 0.6669, 0.0659, 0.5742, 0.9866, 0.3536, 0.1037, 0.9929, 0.4275, 0.3508,\n",
            "        0.4192, 0.1788])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def sigmoid(x):\n",
        "    \"\"\"Sigmoid activation function.\"\"\"\n",
        "    return 1 / (1 + torch.exp(-x))\n",
        "\n",
        "def f(x, a, b, c):\n",
        "    \"\"\"Single TLU function.\"\"\"\n",
        "    # Calculate the weighted sum of input features\n",
        "    z = a * x[:, 0] + b * x[:, 1] + c\n",
        "    # Apply sigmoid activation function\n",
        "    return sigmoid(z)\n",
        "\n",
        "# Generate random input pairs and target vector\n",
        "x = torch.randn((100, 2))\n",
        "y = torch.where(x[:, 0] > x[:, 1], 1.0, 0.0)\n",
        "\n",
        "# Test the single TLU function\n",
        "a = torch.tensor(0.5) # Example weight for x0\n",
        "b = torch.tensor(0.3) # Example weight for x1\n",
        "c = torch.tensor(-0.2) # Example weight for bias term\n",
        "output = f(x, a, b, c)\n",
        "\n",
        "# Print the input pairs, target vector, and output of the single TLU function\n",
        "print(\"Input Pairs:\")\n",
        "print(x)\n",
        "print(\"Target Vector:\")\n",
        "print(y)\n",
        "print(\"Output of Single TLU:\")\n",
        "print(output)\n"
      ],
      "metadata": {
        "id": "6tkoJ_KN7H3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define the sigmoid activation function as sigmoid(x), and the single TLU function f(x, a, b, c) takes input features x (in this case, the generated random input pairs x), and weights a, b, and c. It calculates the weighted sum of the input features using the given weights, and applies the sigmoid activation function to obtain the output. We then test the f function with example weights a, b, and c, and print the input pairs, target vector, and the output of the single TLU function."
      ],
      "metadata": {
        "id": "vDv_Iuw47Hf6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def f(x, params):\n",
        "    a, b, c = params\n",
        "    z = a * x[:, 0] + b * x[:, 1] + c\n",
        "    return F.sigmoid(z)\n",
        "\n",
        "# Example usage\n",
        "x = torch.randn((100, 2))\n",
        "y = torch.where(x[:, 0] > x[:, 1], 1.0, 0.0)\n",
        "params = [3, -2, 1]\n",
        "output = f(x, params)\n",
        "print(output)\n"
      ],
      "metadata": {
        "id": "_EtP5XpK7Qua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this implementation, x is a 2-dimensional torch tensor representing 100 random pairs of numbers, and params is a list of three parameters [a, b, c] that represent the weights of the TLU. The function computes the weighted sum a * x[:, 0] + b * x[:, 1] + c and applies the sigmoid activation function using F.sigmoid() from PyTorch to obtain the output. The resulting output tensor output will have the sigmoid activations for each pair of numbers in x."
      ],
      "metadata": {
        "id": "haxxOsca7WnW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In addition to our TLU function, we need a loss function. Your task is to implement a absolute difference loss function, $∑|x_i-y_i|$, which counts the number of wrong guesses."
      ],
      "metadata": {
        "id": "UBiKkGKx-jVM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def mae(preds, targets):\n",
        "    return torch.sum(torch.abs(preds - targets))\n",
        "\n",
        "# Example usage\n",
        "preds = torch.tensor([0.5, 0.3, 0.8])\n",
        "targets = torch.tensor([1.0, 0.0, 1.0])\n",
        "loss = mae(preds, targets)\n",
        "print(loss)\n"
      ],
      "metadata": {
        "id": "cwzyy281wI7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this implementation, preds and targets are torch tensors representing the predicted values and target values, respectively. The absolute differences between the predicted and target values are computed using torch.abs(), and the sum of these absolute differences is computed using torch.sum() to obtain the MAE loss."
      ],
      "metadata": {
        "id": "o-5Xhg7r7ueq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try to train your single TLU with the absolute difference loss function, use the following code. Choose an appropriate step weight `lr` and try to explain what is happing in each line."
      ],
      "metadata": {
        "id": "eGVNErmbvFxB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 1\n",
        "params = torch.randn(3).requires_grad_()\n",
        "\n",
        "def apply_step(params, prn=True):\n",
        "    preds = f(x, params)\n",
        "    loss = mae(preds, y)\n",
        "    loss.backward()\n",
        "    params.data -= lr * params.grad.data\n",
        "    params.grad = None\n",
        "    if prn: print(params);print(loss.item())\n",
        "    return preds\n",
        "\n",
        "\n",
        "for i in range(50): apply_step(params)"
      ],
      "metadata": {
        "id": "EB5TYTNmyO3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Define the learning rate (lr) to be 1, which determines the step size for updating the parameters during optimization. A higher learning rate means larger updates to the parameters, while a lower learning rate means smaller updates.\n",
        "\n",
        "Initialize the parameters (weights and biases) of the TLU with random values using torch.randn(3).requires_grad_(), and set requires_grad=True to enable gradient computation during training.\n",
        "\n",
        "Define a function apply_step(params, prn=True) which takes in the parameters params and an optional boolean flag prn indicating whether to print the updated parameters and loss at each step.\n",
        "\n",
        "Inside the function, we perform the forward pass by calling the f(x, params) function to obtain the predicted values preds.\n",
        "\n",
        "Compute the MAE loss by calling the mae(preds, y) function, passing in the predicted values and target values y.\n",
        "\n",
        "Perform the backward pass by calling the loss.backward() function to compute gradients of the loss with respect to the parameters.\n",
        "\n",
        "Update the parameters by subtracting the learning rate (lr) multiplied by the gradients of the parameters from the params.data tensor, which contains the current parameter values.\n",
        "\n",
        "Set the gradients of the parameters to None using params.grad = None to reset the gradients for the next iteration.\n",
        "If the prn flag is set to True, we print the updated parameters and loss for monitoring the training progress.\n",
        "\n",
        "Finally, we return the predicted values preds.\n",
        "\n",
        "We enter a loop for training with 50 iterations (as specified by range(50)).\n",
        "\n",
        "Inside the loop, we call the apply_step(params) function to perform a single optimization step, updating the parameters and computing the loss.\n",
        "\n",
        "The updated parameters are printed if the prn flag is set to True, allowing us to monitor the training progress."
      ],
      "metadata": {
        "id": "pVdh-ANv8NxQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a line of code that counts the number of wrong predictions, rounding your predictions with *round()*."
      ],
      "metadata": {
        "id": "h5_LNc1o_o2g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preds = f(x, params)\n",
        "num_wrong = torch.sum(torch.round(preds) != y).item()\n"
      ],
      "metadata": {
        "id": "EEUhyhyDxwMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this line, torch.round(preds) rounds the predicted values to the nearest integer, and torch.round(preds) != y compares the rounded predictions with the target values y element-wise, resulting in a boolean tensor with True where the predictions are wrong and False where the predictions are correct. Finally, torch.sum() is used to count the number of True values in the boolean tensor, and .item() is called to convert the count to a Python integer for convenience."
      ],
      "metadata": {
        "id": "cm4poRPK8zXB"
      }
    }
  ]
}