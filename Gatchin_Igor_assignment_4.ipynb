{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/igorgatchin1993/assigments/blob/main/Gatchin_Igor_assignment_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Theory\n",
        "\n",
        "In the following assignment, your task is to complete the MNIST Basics chapter. It is best to repeat everything from last week and try to answer the following questions. Afterwards you have to summarize the learned facts with two programming tasks.\n",
        "\n",
        "What is \"torch.cat()\" and \".view(-1, 28*28)\" doing in the beginning of the \"The MNIST Loss Function\" chapter?\n",
        "\n",
        "In the beginning of \"The MNIST Loss Function\" chapter, the torch.cat() function is used to concatenate the batch of images into a single tensor.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "What is the defined `mnist_loss` function doing? \n",
        "\n",
        "\n",
        "```\n",
        "def mnist_loss(predictions, targets):\n",
        "    return torch.where(targets==1, 1-predictions, predictions).mean()\n",
        "```"
      ],
      "metadata": {
        "id": "iIBgQ5f43H6z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "The mnist_loss function is a loss function for the task of image classification using the MNIST dataset, which contains grayscale images of hand-written digits (0 to 9). The function takes two arguments:\n",
        "\n",
        "predictions: A tensor of shape (batch_size, num_classes) containing the predicted probabilities for each class for each image in the batch.\n",
        "targets: A tensor of shape (batch_size, num_classes) containing the one-hot encoded target labels for each image in the batch.\n",
        "The function first uses the torch.where function to create a new tensor of the same shape as predictions. For each element of targets that is equal to 1, the corresponding element in the new tensor is set to 1-predictions, and for each element of targets that is equal to 0, the corresponding element in the new tensor is set to predictions.\n",
        "\n",
        "The result of the torch.where function is then averaged over all elements of the tensor using the mean method. This gives the mean cross-entropy loss between the predicted probabilities and the target labels for the batch."
      ],
      "metadata": {
        "id": "58FrvVPOIFDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GrybSPB5IRQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can you draw the neuronal network, which is manually trained in chapter \"The MNIST Loss Function\"?\n",
        "\n",
        "\n",
        "Here is an example code for creating a simple fully connected neural network with one hidden layer and using the mnist_loss function for training:"
      ],
      "metadata": {
        "id": "R0ONUjsxCvhP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define the neural network architecture\n",
        "class NeuralNet(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        self.fc1 = torch.nn.Linear(784, 128)\n",
        "        self.fc2 = torch.nn.Linear(128, 10)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Define the loss function\n",
        "def mnist_loss(predictions, targets):\n",
        "    return torch.where(targets==1, 1-predictions, predictions).mean()\n",
        "\n",
        "# Load the MNIST dataset\n",
        "train_dataset = MNIST(root='./data', train=True, download=True, transform=ToTensor())\n",
        "test_dataset = MNIST(root='./data', train=False, download=True, transform=ToTensor())\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Initialize the neural network and optimizer\n",
        "model = NeuralNet()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "# Train the neural network\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    for images, labels in train_loader:\n",
        "        # Flatten the images into a batch of vectors\n",
        "        images = images.view(-1, 28*28)\n",
        "\n",
        "        # Forward pass\n",
        "        predictions = model(images)\n",
        "\n",
        "        # Compute the loss\n",
        "        targets = torch.zeros(len(labels), 10)\n",
        "        targets[range(len(labels)), labels] = 1\n",
        "        loss = mnist_loss(predictions, targets)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Print the training loss for each epoch\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluate the trained model on the test set\n",
        "with torch.no_grad():\n",
        "    total_correct = 0\n",
        "    total_images = 0\n",
        "    for images, labels in test_loader:\n",
        "        # Flatten the images into a batch of vectors\n",
        "        images = images.view(-1, 28*28)\n",
        "\n",
        "        # Forward pass and compute the predictions\n",
        "        predictions = model(images)\n",
        "        _, predicted = torch.max(predictions, 1)\n",
        "\n",
        "        # Compute the accuracy on the test set\n",
        "        total_correct += (predicted == labels).sum().item()\n",
        "        total_images += labels.size(0)\n",
        "\n",
        "    accuracy = total_correct / total_images\n",
        "    print(f'Test Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "id": "AUzD0DPpCs-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why is it not possible to use the accuracy as loss function?\n",
        "\n",
        "While accuracy is an important metric for evaluating the performance of a machine learning model, it cannot be used as a loss function directly for optimization during the training process. There are several reasons why:\n",
        "\n",
        "1. Non-differentiability: Accuracy is a measure of how well the model predicts the correct class labels, but it is not a differentiable function. This means that small changes in the model parameters cannot be used to calculate a gradient that can be used to update the parameters during training using gradient descent or other optimization algorithms.\n",
        "\n",
        "2. Discrete nature of accuracy: Accuracy is a discrete measure, meaning that it can only take on certain values, such as 0%, 10%, 20%, etc. This makes it difficult to use as a continuous loss function that can be optimized through gradient descent.\n",
        "\n",
        "3. Imbalance of classes: In many classification tasks, the classes may be imbalanced, meaning that there are more examples of one class than the other. In such cases, a model that always predicts the majority class will have a high accuracy but will not be useful. Therefore, other metrics such as precision, recall, F1-score or AUC-ROC, are used to take into account the imbalance of classes and provide a more complete evaluation of the model performance.\n",
        "\n",
        "For these reasons, machine learning models typically use other loss functions such as cross-entropy, mean squared error, or hinge loss, which are differentiable and can be optimized during training to improve the model's performance."
      ],
      "metadata": {
        "id": "_XODZtC5H7W2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is the defined `mnist_loss` function doing? \n",
        "\n",
        "One commonly used loss function for this task is the cross-entropy loss function, which measures the dissimilarity between the predicted class probabilities and the true class labels. Another possible loss function is the mean squared error loss, which measures the average squared difference between the predicted outputs and the true outputs."
      ],
      "metadata": {
        "id": "VSkvmSRrH8QO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why do we need additionaly the sigmoid() function? What is it technically in our TLU?\n",
        "\n",
        "Sigmoid Function in TLU:\n",
        "The sigmoid function is a mathematical function that maps any input value to a value between 0 and 1. In the context of a Threshold Logic Unit (TLU), the sigmoid function is used as an activation function that introduces nonlinearity into the output of the TLU. The TLU performs a weighted sum of its inputs, and the sigmoid function is applied to this sum to produce the output. The sigmoid function can be expressed mathematically as:\n",
        "\n",
        "sigmoid(x) = 1 / (1 + exp(-x))\n",
        "\n",
        "where x is the input to the function. The output of the TLU with the sigmoid activation function can be interpreted as a probability of the output being 1 or 0, depending on the threshold set.\n",
        "\n",
        "\n",
        "\n",
        "Again, what are mini batches, why are we using them and why should they be shuffeld?\n",
        "\n",
        "Mini-Batches:\n",
        "Mini-batch training is a technique used in machine learning to train models using small subsets of the training data at a time. Instead of using the entire training dataset for each iteration of the optimization algorithm, mini-batch training divides the dataset into smaller batches of data, which are then used to update the model parameters. These batches are typically chosen randomly from the dataset, and each batch contains a fixed number of examples. By using mini-batches, the model can be trained more efficiently, as it can update its parameters more frequently, and the stochastic nature of the batch selection can help the model avoid getting stuck in local minima.\n",
        "\n",
        "Shuffling Mini-Batches:\n",
        "Shuffling the mini-batches is important to avoid any systematic patterns that may exist in the data. For example, if the data is sorted by class labels, then using batches that follow this order will lead to a biased model. Shuffling the batches ensures that the model is exposed to a more representative sample of the data in each iteration of the optimization algorithm, which can help to improve the generalization performance of the model. Additionally, shuffling helps to break up any dependencies or correlations that may exist between the examples in the dataset, which can also help to improve the learning process."
      ],
      "metadata": {
        "id": "s60D5xXBIL5u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Practical Part\n",
        "\n",
        "Try to understand all parts of the code needed to manually train a single TLU/Perceptron, so use and copy all parts of the code from \"First Try: Pixel Similarity\" to the \"Putting it all together\" chapter. In the second step, use an optimizer, a second layer, and a ReLU as a hidden activation function to train a simple neural network. When copying the code, think carefully about what you really need and how you can summarize it as compactly as possible. (Probably each attempt requires about 15 lines of code.)"
      ],
      "metadata": {
        "id": "aoQq7z5D3XXV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TxwyNuzj3DYu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "outputId": "deaaff58-d46a-486c-b003-426324bb77b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-baf9dae518ed>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Load the MNIST dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmnist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_openml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mnist_784'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/datasets/_openml.py\u001b[0m in \u001b[0;36mfetch_openml\u001b[0;34m(name, version, data_id, data_home, target_column, cache, return_X_y, as_frame, n_retries, delay, parser)\u001b[0m\n\u001b[1;32m   1078\u001b[0m     \u001b[0;31m# obtain the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1079\u001b[0m     \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_DATA_FILE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_description\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"file_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1080\u001b[0;31m     bunch = _download_data_to_bunch(\n\u001b[0m\u001b[1;32m   1081\u001b[0m         \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1082\u001b[0m         \u001b[0mreturn_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/datasets/_openml.py\u001b[0m in \u001b[0;36m_download_data_to_bunch\u001b[0;34m(url, sparse, data_home, as_frame, openml_columns_info, data_columns, target_columns, shape, md5_checksum, n_retries, delay, parser)\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0mno_retry_exception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParserError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m     X, y, frame, categories = _retry_with_clean_cache(\n\u001b[0m\u001b[1;32m    645\u001b[0m         \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_home\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_retry_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_arff_response\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/datasets/_openml.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mURLError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/datasets/_openml.py\u001b[0m in \u001b[0;36m_load_arff_response\u001b[0;34m(url, data_home, parser, output_type, openml_columns_info, feature_names_to_select, target_names_to_select, shape, md5_checksum, n_retries, delay)\u001b[0m\n\u001b[1;32m    516\u001b[0m     )\n\u001b[1;32m    517\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m         X, y, frame, categories = _open_url_and_load_gzip_file(\n\u001b[0m\u001b[1;32m    519\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_home\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_retries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marff_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/datasets/_openml.py\u001b[0m in \u001b[0;36m_open_url_and_load_gzip_file\u001b[0;34m(url, data_home, n_retries, delay, arff_params)\u001b[0m\n\u001b[1;32m    505\u001b[0m         \u001b[0mgzip_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_openml_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_home\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_retries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_retries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdelay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mclosing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgzip_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 507\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mload_arff_from_gzip_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgzip_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0marff_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     arff_params = dict(\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/datasets/_arff_parser.py\u001b[0m in \u001b[0;36mload_arff_from_gzip_file\u001b[0;34m(gzip_file, parser, output_type, openml_columns_info, feature_names_to_select, target_names_to_select, shape, read_csv_kwargs)\u001b[0m\n\u001b[1;32m    508\u001b[0m     \"\"\"\n\u001b[1;32m    509\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mparser\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"liac-arff\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m         return _liac_arff_parser(\n\u001b[0m\u001b[1;32m    511\u001b[0m             \u001b[0mgzip_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m             \u001b[0moutput_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/datasets/_arff_parser.py\u001b[0m in \u001b[0;36m_liac_arff_parser\u001b[0;34m(gzip_file, output_arrays_type, openml_columns_info, feature_names_to_select, target_names_to_select, shape)\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0mcolumns_to_keep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcolumns_names\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcolumns_to_select\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0mdfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfirst_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumns_to_keep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_chunk_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marff_container\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m             \u001b[0mdfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumns_to_keep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m_chunk_generator\u001b[0;34m(gen, chunksize)\u001b[0m\n\u001b[1;32m    719\u001b[0m     chunk may have a length less than ``chunksize``.\"\"\"\n\u001b[1;32m    720\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 721\u001b[0;31m         \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mislice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    722\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/externals/_arff.py\u001b[0m in \u001b[0;36mdecode_rows\u001b[0;34m(self, stream, conversors)\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode_rows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconversors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parse_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/externals/_arff.py\u001b[0m in \u001b[0;36m_parse_values\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0;31m# Fast path for trivial cases (unfortunately we have to handle missing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;31m# values because of the empty string case :(.)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m         return [None if s in ('?', '') else s\n\u001b[0m\u001b[1;32m    289\u001b[0m                 for s in next(csv.reader([s]))]\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/externals/_arff.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0;31m# Fast path for trivial cases (unfortunately we have to handle missing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;31m# values because of the empty string case :(.)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m         return [None if s in ('?', '') else s\n\u001b[0m\u001b[1;32m    289\u001b[0m                 for s in next(csv.reader([s]))]\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#YOUR TASK: Manually train a single layer perceptron without using an optimizer.\n",
        "import numpy as np\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the MNIST dataset\n",
        "mnist = fetch_openml('mnist_784')\n",
        "X, y = mnist['data'], mnist['target']\n",
        "\n",
        "# Preprocess the data\n",
        "X = X.astype(np.float32) / 255.0\n",
        "y = y.astype(np.int32)\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the model architecture\n",
        "n_inputs = X_train.shape[1]\n",
        "n_outputs = 10  # number of classes\n",
        "weights = np.random.randn(n_inputs, n_outputs)\n",
        "bias = np.zeros(n_outputs)\n",
        "\n",
        "# Define the hyperparameters\n",
        "learning_rate = 0.1\n",
        "n_epochs = 50\n",
        "batch_size = 32\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(n_epochs):\n",
        "    # Shuffle the training data\n",
        "    idx = np.random.permutation(len(X_train))\n",
        "    X_train_shuffled = X_train[idx]\n",
        "    y_train_shuffled = y_train[idx]\n",
        "    \n",
        "    # Loop over mini-batches\n",
        "    for i in range(0, len(X_train_shuffled), batch_size):\n",
        "        X_batch = X_train_shuffled[i:i+batch_size]\n",
        "        y_batch = y_train_shuffled[i:i+batch_size]\n",
        "        \n",
        "        # Compute the forward pass\n",
        "        z = np.dot(X_batch, weights) + bias\n",
        "        y_pred = np.argmax(z, axis=1)\n",
        "        \n",
        "        # Compute the loss\n",
        "        loss = np.mean(y_batch != y_pred)\n",
        "        \n",
        "        # Compute the backward pass\n",
        "        grad_z = (y_pred - y_batch).reshape(-1, 1)\n",
        "        grad_weights = np.dot(X_batch.T, grad_z)\n",
        "        grad_bias = np.sum(grad_z, axis=0)\n",
        "        \n",
        "        # Update the weights and bias\n",
        "        weights -= learning_rate * grad_weights\n",
        "        bias -= learning_rate * grad_bias\n",
        "        \n",
        "    # Compute the validation accuracy\n",
        "    z_val = np.dot(X_val, weights) + bias\n",
        "    y_val_pred = np.argmax(z_val, axis=1)\n",
        "    val_acc = np.mean(y_val == y_val_pred)\n",
        "    \n",
        "    print('Epoch: {}, Loss: {:.4f}, Val Acc: {:.4f}'.format(epoch, loss, val_acc))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code defines a single-layer perceptron with weights and bias initialized randomly, and trains it on the MNIST dataset using mini-batch stochastic gradient descent. The code shuffles the training data at the beginning of each epoch to ensure the model sees a representative sample of the data in each iteration. The loss is computed as the mean classification error between the predicted and true labels. The weights and bias are updated using the gradients computed by the backward pass. The validation accuracy is computed at the end of each epoch to monitor the generalization performance of the model."
      ],
      "metadata": {
        "id": "RG7GE6HsMVnc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#YOUR TASK: Train a simple two-layer neural network (two perceptrons + hidden activation function) with built-in functions and an optimizer.\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Define the network architecture\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 64) # input layer to hidden layer\n",
        "        self.fc2 = nn.Linear(64, 10) # hidden layer to output layer\n",
        "        self.relu = nn.ReLU() # activation function\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x.view(x.shape[0], -1) # flatten the input tensor\n",
        "        x = self.relu(self.fc1(x)) # pass through hidden layer with ReLU activation function\n",
        "        x = self.fc2(x) # pass through output layer\n",
        "        return x\n",
        "\n",
        "# Load the MNIST dataset\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5,), (0.5,))])\n",
        "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "# Train the network\n",
        "net = Net()\n",
        "for epoch in range(5): # number of epochs\n",
        "    running_loss = 0\n",
        "    for images, labels in trainloader: # loop through mini-batches\n",
        "        optimizer.zero_grad() # zero the gradients\n",
        "        output = net(images) # forward pass\n",
        "        loss = criterion(output, labels) # calculate loss\n",
        "        loss.backward() # backward pass\n",
        "        optimizer.step() # update weights\n",
        "        running_loss += loss.item() # calculate running loss\n",
        "        \n",
        "    print(f\"Epoch {epoch+1} - Training loss: {running_loss/len(trainloader)}\")\n"
      ],
      "metadata": {
        "id": "UGsLRFtMbyRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code defines a Net class for the network architecture, which consists of two fully connected layers with a ReLU activation function in between. The code then loads the MNIST dataset and defines the loss function and optimizer. Finally, the network is trained using a for loop over the number of epochs and a nested for loop over mini-batches. The optimizer is used to update the weights of the network during training."
      ],
      "metadata": {
        "id": "xWxNneFkMa9N"
      }
    }
  ]
}